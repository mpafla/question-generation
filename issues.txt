word2vec:
	Trained with own data -> not as good
	This works: Use Google's model, create embeddings matrix but add another dimension for each special token (https://stackoverflow.com/questions/41881605/what-should-be-the-word-vectors-of-token-pad-unknown-go-eos-before-se)
	Problem: Cannot include embeddings matrix in keras Embedding because of tf.function

likelihood as a decoding objective (greedy):
	repetitive and strange
	try beam-search (but sensitive to length of output (works well with translation where length of output follows input)
	try top-k or nucleus sampling
	

attention:

data_generation:
	change data format in tensorflow liked format and not pickle
