word2vec:
	Trained with own data -> not as good
	Use Google's w2v -> don't know how to include padding or unk
	Potential solution: Use Google's model, create embeddings matrix but add another dimension for each special token (https://stackoverflow.com/questions/41881605/what-should-be-the-word-vectors-of-token-pad-unknown-go-eos-before-se)

likelihood as a decoding objective (greedy):
	repetitive and strange
	try beam-search (but sensitive to length of output (works well with translation where length of output follows input)
	try top-k or nucleus sampling
	

attention:
